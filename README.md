
---

```markdown
# Image Caption Generator

A project that generates descriptive captions for images using deep learning, combining CNN-based image feature extraction with RNN/LSTM (or Transformer-based) language modeling.

## ğŸš€ Overview

This repository provides a pipeline to train and deploy an image captioning model:

- **Feature Extraction**: Uses a CNN architecture (e.g. VGG16, ResNet, or ViT) to encode images.
- **Caption Generation**: Employs an RNN (e.g., LSTM) or Transformer decoder to generate natural-language captions.
- **Optional Fineâ€‘tuning**: Support for pre-trained image-language models (e.g. ViTâ€‘GPT2 style) for improved accuracy.

## ğŸ§  Key Features

- Preprocess images and caption annotations (e.g. from Flickr8k, MSâ€‘COCO).
- Tokenize captions and prepare training sequences.
- Train encoderâ€“decoder architecture with optional attention mechanism.
- Generate captions for unseen images.
- Optionally deploy via a Flask or Streamlit web interface for interactive use.

## ğŸ“ Repository Structure

```

â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/             # Input images (Flickr8k, COCO, etc.)
â”‚   â”œâ”€â”€ captions.txt        # Caption annotations
â”‚   â””â”€â”€ processed/          # Tokenized captions, vocabulary, data splits
â”œâ”€â”€ model/                  # Pretrained checkpoints and saved models
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract\_features.py # CNN feature extractor
â”‚   â”œâ”€â”€ train.py            # Model training script
â”‚   â”œâ”€â”€ inference.py        # Caption generation script
â”‚   â”œâ”€â”€ utils.py            # Dataset & vocabulary utilities
â”‚   â””â”€â”€ web\_app.py          # Flask/Streamlit application (if included)
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md               # This file
â””â”€â”€ LICENSE                 # Project license

````

## ğŸ”§ Requirements & Installation

```bash
git clone https://github.com/AbiramReddySatti/Image-Caption-Generator.git
cd Image-Caption-Generator
python3 -m venv venv
source venv/bin/activate    # or `venv\Scripts\activate` on Windows
pip install -r requirements.txt
````

Required packages typically include:

* `torch`, `tensorflow` or `keras`
* `transformers` (if using pretrained models)
* `pillow`
* `numpy`, `pandas`
* `flask` or `streamlit` for web demo (optional)

## ğŸ› ï¸ Preparing Data

1. Download an image-caption dataset (e.g., Flickr8k, MSâ€‘COCO).
2. Place images in `data/images/` and captions in `data/captions.txt`.
3. Edit paths or config variables in `src/utils.py` or `train.py`.
4. Run preprocessing:

   ```bash
   python src/utils.py --preprocess
   ```

   * Cleans textual captions, builds token vocabulary, and creates training/test splits.

## ğŸ§ª Training the Model

```bash
python src/train.py \
  --features_dir data/processed/features \
  --captions data/processed/captions.pkl \
  --model_dir model/ \
  --batch_size 64 \
  --epochs 20
```

Training saves model checkpoints, training logs, and optionally evaluation metrics like BLEU or CIDEr.

## ğŸ¤– Generating Captions

```bash
python src/inference.py \
  --image_path sample.jpg \
  --model_path model/best_model.pth \
  --vocab data/processed/vocab.pkl
```

Outputs a caption like:

> â€œa group of people sitting around a table with foodâ€

## ğŸŒ Optional: Web Interface

To try live captioning:

### Using Streamlit:

```bash
streamlit run src/web_app.py
```

### Or Flask:

```bash
python src/web_app.py
```

Upload an image and view generated captions in your browser.

## ğŸ“ˆ Evaluation (Optional)

Evaluate model performance using metrics such as BLEUâ€‘1 to BLEUâ€‘4, ROUGEâ€‘L, CIDErâ€‘D. Use COCO-style evaluation scripts or library functions supported in your training pipeline.

## ğŸ“˜ Based on Established Methods

This architecture draws from classic encoder-decoder approaches such as â€œShow and Tellâ€ by Vinyals et al. (2014) ([github.com][1], [github.com][2], [geeksforgeeks.org][3], [ijsred.com][4], [youtube.com][5], [aclanthology.org][6]). For better performance, more recent approaches like CLIP + GPTâ€‘2 style captioning (e.g. `nlpconnect/vitâ€‘gpt2â€‘imageâ€‘captioning`) can be used ([michael-franke.github.io][7]).

## ğŸ”„ Potential Enhancements

* Use attention mechanisms (e.g. Bahdanau or Lu et al.â€™s adaptive attention).
* Replace feature extractor with transformer-based ViT encoders.
* Add beam search decoding for multiple caption candidates.
* Support multilingual captioning (e.g. via fine-tuned multilingual models or cross-lingual datasets).
* Add inference confidence, caption diversity, or grammar smoothing.

## ğŸ§° License & Contributions

Please see the `LICENSE` file for license details (e.g., MIT). Contributions are welcomeâ€”feel free to open issues or pull requests.

---
